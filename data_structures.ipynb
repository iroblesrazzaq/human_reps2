{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File to create and test data structures for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mat73\n",
    "from scipy.io import loadmat\n",
    "import re\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### modify this, will need more specificity\n",
    "### so essentially we will have recording info classes in patient data for movie, preSleepFR, postSleepFR\n",
    "### we will have a method to take the recording from just the start/end unix or relative time lol\n",
    "### more robust to just deal with absolute unix? we will have to make relative at some point\n",
    "### better for the PatientData class to have relative information - dicionary of movie, preSleep, postSleep FR times?\n",
    "### or should I have movie, etc hardcoded? better to have recordinginfo class likely\n",
    "\n",
    "@dataclass\n",
    "class RecordingInfo:\n",
    "    \"\"\"Contains metadata about a recording session\"\"\"\n",
    "    start_unix: float\n",
    "    end_unix: float\n",
    "    experiment_type: str  # e.g., 'movie', 'preSleep', 'postSleep'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Neuron:\n",
    "    \"\"\"\n",
    "    Class for a single Neuron in a single patient\n",
    "    pid: str pid of patient (redundancy maybe but better safe than sorry)\n",
    "    area: str | None  recording area\n",
    "    spikes: list | np.ndarray all spike times (in seconds)\n",
    "    \"\"\"\n",
    "    neuron_id: str\n",
    "    pid: str\n",
    "    spike_times: np.ndarray\n",
    "    area: str | None = None\n",
    "    metadata: dict | None = None\n",
    "    \n",
    "    @property\n",
    "    def firing_rate(self, window: Tuple[float, float] = None) -> float:\n",
    "        \"\"\"\n",
    "        Function to get firing rate of neuron within a certain time period (default whole recording)\n",
    "\n",
    "        times are in seconds\n",
    "        \"\"\"\n",
    "        if window:\n",
    "            spikes = self.spike_times[(self.spike_times >= window[0]) & (self.spike_times <= window[1])]\n",
    "            duration = window[1] - window[0]\n",
    "        else:\n",
    "            spikes = self.spike_times\n",
    "            duration = self.spike_times[-1] - self.spike_times[0]\n",
    "\n",
    "        assert duration > 0, \"Duration < 0, Error\"\n",
    "\n",
    "        return len(spikes) / duration\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientData:\n",
    "    \"\"\"\n",
    "    Contains all relevant information for a single patient\n",
    "\n",
    "    - movie drift adjusted times with CSV (create new csv to use every time or run code every time -- not expensive so will do second for reproducibility)\n",
    "    - patient info from all exp epochs (start unix, pre/post, etc etc)\n",
    "    - recordings!!  \n",
    "        - dictionary with all neurons and firing times? but also want to be able to filter by brain area\n",
    "\n",
    "\n",
    "    - methods for analysis?\n",
    "        - can be functions for general, not specific to patient\n",
    "        - make analysis class?\n",
    "            - would have functions for heatmaps, decoders, etc?\n",
    "        - these will clutter patient class, I mainly just want all data for a single patient concentrated in one place, easy to use and access\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pid: str):\n",
    "        # want to load csv, fix correlation issue\n",
    "        # so call function to load csv, multiple times by coefficient, \n",
    "        # get concept onsets from the \n",
    "        self.pid = pid\n",
    "\n",
    "\n",
    "        d = Dataloader() # type: ignore\n",
    "\n",
    "        self.neurons: list[Neuron] = d.get_all_patient_neurons(self.pid)\n",
    "\n",
    "\n",
    "    def _load_data(self) -> None:\n",
    "        \"\"\"\n",
    "        takes pid, loads spike data, json timing info data, concept onset data\n",
    "            spike data function\n",
    "                - want all neurons recorded with a list of times that they spike relative to recording start\n",
    "                - go through all mat files, create instances of Neuron class for each Neuron\n",
    "                    - list of neuron class\n",
    "                - neuron class \n",
    "                    - area\n",
    "                    - spike times list/array\n",
    "                    - firing rate method - start/end time optional argument\n",
    "                    \n",
    "            json timing info function\n",
    "            concept onset function\n",
    "                - adjust timing csv with correlation factor in json\n",
    "                - get relative times for each recall concept onset\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "brainstorming of patient class, has good thoughts\n",
    "\n",
    "\n",
    "    Contains all relevant information for a single patient\n",
    "\n",
    "    - movie drift adjusted times with CSV (create new csv to use every time or run code every time -- not expensive so will do second for reproducibility)\n",
    "    - patient info from all exp epochs (start unix, pre/post, etc etc)\n",
    "    - recordings!!  \n",
    "        - dictionary with all neurons and firing times? but also want to be able to filter by brain area\n",
    "\n",
    "\n",
    "    - methods for analysis?\n",
    "        - can be functions for general, not specific to patient\n",
    "        - make analysis class?\n",
    "            - would have functions for heatmaps, decoders, etc?\n",
    "        - these will clutter patient class, I mainly just want all data for a single patient concentrated in one place, easy to use and access\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "    \"\"\"Class to contain functions to load data\"\"\"\n",
    "\n",
    "\n",
    "    def parse_filename(self, filename):\n",
    "        base = filename.split('-')[-1].replace('.mat', '')\n",
    "\n",
    "        parsed = filename.replace('.mat', '').split('-')\n",
    "        if len(parsed) == 2:  # Normal case like GA2-RAH7\n",
    "            base = parsed[-1]\n",
    "        elif len(parsed) == 3:  # Case with hyphenated area like GA3-RSUB-PHG1\n",
    "            base = '-'.join(parsed[1:])  # Join with hyphen to preserve structure\n",
    "        else:\n",
    "            return (filename.replace('.mat', ''), None)\n",
    "\n",
    "        match = re.match(r'(.*?[-]?\\w+?)(\\d+)$', base)\n",
    "        if match:\n",
    "            area_name = match.group(1)  # Group 1 contains everything before the numbers\n",
    "            channel_num = match.group(2)  # Group 2 contains the numbers\n",
    "            return base, area_name\n",
    "        return (base, None) # None for no areaname\n",
    "    \n",
    "\n",
    "    def _get_neurons_from_mat(self, file_path, pid):\n",
    "        \"\"\"\n",
    "        Load spike data from .mat file, handling different MATLAB file versions\n",
    "\n",
    "        Return instances of the Neuron class, adding spike data to each one\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data = loadmat(file_path)\n",
    "        except (NotImplementedError, TypeError):\n",
    "            data = mat73.loadmat(file_path)\n",
    "        \n",
    "        # Extract cluster_class data\n",
    "        cluster_class = data['cluster_class']\n",
    "        \n",
    "        # Extract timestampsStart\n",
    "        ts_start = data[\"timestampsStart\"]\n",
    "        if ts_start.shape == ():\n",
    "            ts_start = float(ts_start)\n",
    "        else:\n",
    "            ts_start = float(ts_start[0][0])\n",
    "\n",
    "        filename = file_path.split('/')[-1]\n",
    "        base, area_name = self.parse_filename(filename)\n",
    "        neurons = []\n",
    "        unique_clusters = np.unique(cluster_class[:, 0])\n",
    "        for cluster_id in unique_clusters:\n",
    "            mask = cluster_class[:, 0] == cluster_id\n",
    "            spike_times = cluster_class[mask, 1]\n",
    "\n",
    "            neurons.append(Neuron(\n",
    "                neuron_id=f\"{base}-{int(cluster_id)}\",\n",
    "                pid = pid,\n",
    "                spike_times=spike_times,\n",
    "                area=area_name,\n",
    "                metadata={'ts_start': ts_start}\n",
    "                ))\n",
    "\n",
    "        return neurons\n",
    "    \n",
    "    def get_all_patient_neurons(self, pid, base_dir=\"./Data\"):\n",
    "        neurons = []\n",
    "        for patient_dir in os.listdir(base_dir): # lists 566_movie paradigm, etc dirs\n",
    "            patient_dict_name = f\"{patient_dir.replace('_MovieParadigm', '')}_files\"\n",
    "\n",
    "            if pid in patient_dir: # we have the correct patient id\n",
    "                for exp_dir in os.listdir(os.path.join(base_dir, patient_dir)):\n",
    "                    if len(exp_dir.split('-')) > 2: # then we have our exp-5-6-7 pattern directory with spiking files\n",
    "                        for file in os.listdir(os.path.join(base_dir, patient_dir, exp_dir, 'CSC_micro_spikes')):\n",
    "                            file_path = os.path.join(base_dir, patient_dir, exp_dir, 'CSC_micro_spikes', file)\n",
    "                            neurons += self._get_neurons_from_mat(file_path=file_path, pid=pid)\n",
    "        return neurons # list of all neurons\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def package_data(base_dir):\n",
    "    # altering to be method to go through all mat files and make instances of neuron class for each neuron\n",
    "    \"\"\"\n",
    "    Input: base directory\n",
    "    \n",
    "    Returns: area_dict -- dict[str, list[tuple[array, float]]]\n",
    "    \"\"\"\n",
    "    \n",
    "    area_dict = {}\n",
    "    for filename in os.listdir(base_dir):\n",
    "        if not filename.startswith('times_manual_') or not filename.endswith('.mat'):\n",
    "            continue\n",
    "            \n",
    "        file_info = parse_filename(filename)\n",
    "        if not file_info:\n",
    "            continue\n",
    "        base, area_name = file_info\n",
    "\n",
    "        full_filename = os.path.join(base_dir, filename)\n",
    "        cluster_class, ts_start = load_spike_data(full_filename)\n",
    "        \n",
    "        if area_name in area_dict:\n",
    "            area_dict[area_name].append((cluster_class, ts_start))\n",
    "        else:\n",
    "            area_dict[area_name] = [(cluster_class, ts_start)]\n",
    "    return area_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39582,)\n",
      "(214701,)\n",
      "(17156,)\n",
      "(47242,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3.25699902e+00, 3.32734251e+00, 3.45099878e+00, ...,\n",
       "       4.13640798e+04, 4.13640838e+04, 4.13640864e+04])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./Data/562_MovieParadigm/Experiment-5-6-7/CSC_micro_spikes/times_manual_GA1-RAI2.mat\"\n",
    "cluster_class, ts_start = load_spike_data(path)\n",
    "cluster_class[100:130, :]\n",
    "np.max(cluster_class, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAI2 RAI\n"
     ]
    }
   ],
   "source": [
    "base, area_name = parse_filename(path.split('/')[-1])\n",
    "print(base, area_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron(neuron_id='LSTG6-1', pid='566', spike_times=array([3.49606180e+00, 4.35087299e+00, 4.86249804e+00, ...,\n",
      "       4.44968594e+04, 4.44968747e+04, 4.44969399e+04]), area='LSTG', metadata={'ts_start': 1691269348.894448})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "169"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = Dataloader()\n",
    "pid = '566'\n",
    "res = d._get_all_patient_neurons(pid=pid)\n",
    "n1 = res[1]\n",
    "print(n1)\n",
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids = ['562', '563', '566']\n",
    "patients = {}\n",
    "for pid in pids:\n",
    "    patients[pid] = PatientData(pid=pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

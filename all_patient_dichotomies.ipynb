{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1773fa26",
   "metadata": {},
   "source": [
    "### File for 4x4 concept decoding across all patients, 10 iterations each\n",
    "Note - not going to use MTL neurons only b/c other patients don't have enough MTL neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae76bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from tqdm import tqdm\n",
    "from data_structures import PatientData\n",
    "from decoders import ConceptDecoder, SingleResultsManager\n",
    "from sklearn.svm import LinearSVC\n",
    "from typing import defaultdict\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7123bf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory structure if it doesn't exist\n",
    "RESULTS_DIR = \"./decoding_res_4x4_april9\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c333b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_consistent_combinations(strings_list, group_size):\n",
    "    all_combos = list(combinations(strings_list, group_size))\n",
    "    unique_pairs = []\n",
    "    seen = set()\n",
    "    \n",
    "    for combo in all_combos:\n",
    "        # Find the complement (the items not in this combination)\n",
    "        complement = tuple(c for c in strings_list if c not in combo)\n",
    "        \n",
    "        # Order consistently to avoid duplicates\n",
    "        if combo < complement:\n",
    "            pair = (combo, complement)\n",
    "        else:\n",
    "            pair = (complement, combo)\n",
    "        \n",
    "        pair_str = str(pair)\n",
    "        if pair_str not in seen:\n",
    "            unique_pairs.append(pair)\n",
    "            seen.add(pair_str)\n",
    "    \n",
    "    return unique_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d8cbc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_concepts = [\n",
    "    \"A.Fayed\", \"R.Wallace\", \"T.Lennox\", \"N.Yassir\", \n",
    "    \"K.Hayes\", \"M.OBrian\", \"J.Bauer\", \"C.Manning\"\n",
    "]\n",
    "stable_groups = generate_consistent_combinations(best_concepts, 4)\n",
    "\n",
    "THRESHOLD = 0.1  # firing rate threshold\n",
    "MAX_ITERATIONS = 10  # Total iterations per group/patient combination\n",
    "PATIENT_IDS = ['562', '563', '566']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0bdecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patient_dict(pids):\n",
    "    \"\"\"Create and return dictionary with patient data and filtered neurons\"\"\"\n",
    "    patient_dict = {}\n",
    "    \n",
    "    for pid in pids:\n",
    "        print(f\"Loading data for patient {pid}...\")\n",
    "        # Create PatientData object\n",
    "        patient = PatientData(pid=pid)\n",
    "        \n",
    "        # Filter neurons by firing rate\n",
    "        fr_neurons = patient.filter_neurons_by_fr(\n",
    "            neurons=patient.neurons, \n",
    "            window=(patient.times_dict['movie_start_rel'], patient.times_dict['preSleep_recall_start_rel']), \n",
    "            threshold=THRESHOLD\n",
    "        )\n",
    "        \n",
    "        # Store as [PatientData object, filtered neurons list]\n",
    "        patient_dict[pid] = [patient, fr_neurons]\n",
    "        print(f\"Patient {pid}: {len(fr_neurons)} neurons after filtering\")\n",
    "    \n",
    "    return patient_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e57cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a unique run ID using timestamp\n",
    "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_DIR = os.path.join(RESULTS_DIR, f\"run_{RUN_ID}\")\n",
    "os.makedirs(RUN_DIR, exist_ok=True)\n",
    "\n",
    "config = {\n",
    "    'concepts': best_concepts,\n",
    "    'threshold': THRESHOLD,\n",
    "    'max_iterations': MAX_ITERATIONS,\n",
    "    'patient_ids': PATIENT_IDS,\n",
    "    'timestamp': RUN_ID,\n",
    "    'num_concept_groups': len(stable_groups)\n",
    "}\n",
    "joblib.dump(config, os.path.join(RUN_DIR, \"config.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe5cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_filename(pid, group_idx, iteration):\n",
    "    \"\"\"Generate a standardized filename for saving results\"\"\"\n",
    "    return os.path.join(RUN_DIR, f\"patient_{pid}_group_{group_idx}_iter_{iteration}.pkl\")\n",
    "\n",
    "# Define a function to check if a result file already exists\n",
    "def result_exists(pid, group_idx, iteration):\n",
    "    \"\"\"Check if a result file already exists\"\"\"\n",
    "    filename = get_result_filename(pid, group_idx, iteration)\n",
    "    return os.path.exists(filename)\n",
    "\n",
    "# Define a function to save progress metadata\n",
    "def save_progress(pid_dict, completed_dict):\n",
    "    \"\"\"Save progress metadata\"\"\"\n",
    "    progress = {\n",
    "        'completed': completed_dict,\n",
    "        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    joblib.dump(progress, os.path.join(RUN_DIR, \"progress.pkl\"))\n",
    "\n",
    "# Define a function to load progress metadata\n",
    "def load_progress():\n",
    "    \"\"\"Load progress metadata if it exists\"\"\"\n",
    "    progress_file = os.path.join(RUN_DIR, \"progress.pkl\")\n",
    "    if os.path.exists(progress_file):\n",
    "        return joblib.load(progress_file)\n",
    "    else:\n",
    "        return {'completed': {}, 'timestamp': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93133f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_decoding_with_checkpoints():\n",
    "    # Load or initialize patient data\n",
    "    patient_dict = create_patient_dict(PATIENT_IDS)\n",
    "    \n",
    "    # Initialize or load progress\n",
    "    progress = load_progress()\n",
    "    completed_dict = progress['completed']\n",
    "    \n",
    "    # Track overall progress\n",
    "    total_tasks = len(PATIENT_IDS) * len(stable_groups) * MAX_ITERATIONS\n",
    "    completed_tasks = sum(len(iterations) for iterations in completed_dict.values())\n",
    "    \n",
    "    print(f\"Starting decoding run: {RUN_ID}\")\n",
    "    print(f\"Total tasks: {total_tasks}\")\n",
    "    print(f\"Completed tasks: {completed_tasks}\")\n",
    "    print(f\"Progress: {(completed_tasks/total_tasks)*100:.2f}%\")\n",
    "    \n",
    "    # Main loop over all patients, groups, and iterations\n",
    "    for pid in PATIENT_IDS:\n",
    "        # Get patient data and filtered neurons\n",
    "        patient, neurons = patient_dict[pid]\n",
    "        \n",
    "        # Make sure this patient has a tracking entry\n",
    "        if pid not in completed_dict:\n",
    "            completed_dict[pid] = {}\n",
    "        \n",
    "        for group_idx, group_pair in enumerate(stable_groups):\n",
    "            group_key = str(group_idx)\n",
    "            \n",
    "            # Initialize this group's tracking if needed\n",
    "            if group_key not in completed_dict[pid]:\n",
    "                completed_dict[pid][group_key] = []\n",
    "            \n",
    "            # Get completed iterations for this combination\n",
    "            completed_iterations = completed_dict[pid][group_key]\n",
    "            \n",
    "            # Skip if all iterations completed\n",
    "            if len(completed_iterations) >= MAX_ITERATIONS:\n",
    "                print(f\"All iterations completed for patient {pid}, group {group_idx}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate remaining iterations\n",
    "            remaining_iterations = MAX_ITERATIONS - len(completed_iterations)\n",
    "            \n",
    "            print(f\"Processing patient {pid}, group {group_idx}: {remaining_iterations} iterations remaining\")\n",
    "            \n",
    "            # Create results manager for this group\n",
    "            manager = SingleResultsManager(\n",
    "                patient_data=patient,\n",
    "                concept_items=[group_pair],  # List with a single group pair\n",
    "                epoch='movie',\n",
    "                classifier=LinearSVC(random_state=42),\n",
    "                standardize=True,\n",
    "                pseudo=True,  # Use pseudopopulations for balanced datasets\n",
    "                neurons=neurons\n",
    "            )\n",
    "            \n",
    "            # Run iterations one by one, saving each result\n",
    "            for i in range(remaining_iterations):\n",
    "                iter_num = len(completed_iterations) + i\n",
    "                print(f\"  Running iteration {iter_num + 1}/{MAX_ITERATIONS}\")\n",
    "                \n",
    "                # Run a single iteration\n",
    "                start_time = time.time()\n",
    "                manager.run_decoding(num_iter=1)\n",
    "                end_time = time.time()\n",
    "                \n",
    "                # Save the result\n",
    "                if manager.results:\n",
    "                    result = manager.results[group_pair][0]  # Get the first (only) result\n",
    "                    filename = get_result_filename(pid, group_idx, iter_num)\n",
    "                    \n",
    "                    # Add metadata to result\n",
    "                    result_with_meta = {\n",
    "                        'result': result,\n",
    "                        'patient_id': pid,\n",
    "                        'group_idx': group_idx,\n",
    "                        'group_pair': group_pair,\n",
    "                        'iteration': iter_num,\n",
    "                        'duration': end_time - start_time,\n",
    "                        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    }\n",
    "                    \n",
    "                    # Save result with joblib\n",
    "                    joblib.dump(result_with_meta, filename)\n",
    "                    \n",
    "                    # Update progress tracking\n",
    "                    completed_dict[pid][group_key].append(iter_num)\n",
    "                    save_progress(patient_dict, completed_dict)\n",
    "                    \n",
    "                    print(f\"  Iteration completed in {end_time - start_time:.2f} seconds\")\n",
    "                else:\n",
    "                    print(f\"  No results for this iteration, possibly due to insufficient data\")\n",
    "            \n",
    "    print(\"Decoding completed!\")\n",
    "    return completed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3622a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(run_dir=None):\n",
    "    \"\"\"Load all results from a run directory\"\"\"\n",
    "    if run_dir is None:\n",
    "        # Use most recent run if none specified\n",
    "        all_runs = sorted([d for d in os.listdir(RESULTS_DIR) if d.startswith(\"run_\")], reverse=True)\n",
    "        if not all_runs:\n",
    "            print(\"No runs found\")\n",
    "            return None\n",
    "        run_dir = os.path.join(RESULTS_DIR, all_runs[0])\n",
    "    \n",
    "    # Load configuration\n",
    "    config = joblib.load(os.path.join(run_dir, \"config.pkl\"))\n",
    "    print(f\"Loaded configuration from {run_dir}\")\n",
    "    print(f\"Run timestamp: {config['timestamp']}\")\n",
    "    print(f\"Patients: {config['patient_ids']}\")\n",
    "    print(f\"Concept groups: {config['num_concept_groups']}\")\n",
    "    \n",
    "    # Get all result files\n",
    "    result_files = [f for f in os.listdir(run_dir) if f.endswith(\".pkl\") and not f.startswith(\"config\") and not f.startswith(\"progress\")]\n",
    "    print(f\"Found {len(result_files)} result files\")\n",
    "    \n",
    "    # Load all results into a structured dictionary\n",
    "    results = {}\n",
    "    for filename in result_files:\n",
    "        filepath = os.path.join(run_dir, filename)\n",
    "        result_data = joblib.load(filepath)\n",
    "        \n",
    "        pid = result_data['patient_id']\n",
    "        group_idx = result_data['group_idx']\n",
    "        iter_num = result_data['iteration']\n",
    "        \n",
    "        if pid not in results:\n",
    "            results[pid] = {}\n",
    "        if group_idx not in results[pid]:\n",
    "            results[pid][group_idx] = []\n",
    "        \n",
    "        results[pid][group_idx].append((iter_num, result_data))\n",
    "    \n",
    "    return results, config\n",
    "\n",
    "# Function to resume an incomplete run\n",
    "def resume_decoding():\n",
    "    \"\"\"Resume the most recent decoding run if it's incomplete\"\"\"\n",
    "    # Find the most recent run\n",
    "    all_runs = sorted([d for d in os.listdir(RESULTS_DIR) if d.startswith(\"run_\")], reverse=True)\n",
    "    if not all_runs:\n",
    "        print(\"No previous runs found. Starting new run.\")\n",
    "        return run_decoding_with_checkpoints()\n",
    "    \n",
    "    most_recent_run = os.path.join(RESULTS_DIR, all_runs[0])\n",
    "    progress_file = os.path.join(most_recent_run, \"progress.pkl\")\n",
    "    \n",
    "    if not os.path.exists(progress_file):\n",
    "        print(\"Most recent run has no progress file. Starting new run.\")\n",
    "        return run_decoding_with_checkpoints()\n",
    "    \n",
    "    # Load progress and configuration\n",
    "    progress = joblib.load(progress_file)\n",
    "    config = joblib.load(os.path.join(most_recent_run, \"config.pkl\"))\n",
    "    \n",
    "    # Check if all tasks are completed\n",
    "    completed_dict = progress['completed']\n",
    "    total_tasks = len(config['patient_ids']) * config['num_concept_groups'] * config['max_iterations']\n",
    "    completed_tasks = sum(len(iterations) for pid_dict in completed_dict.values() for iterations in pid_dict.values())\n",
    "    \n",
    "    if completed_tasks >= total_tasks:\n",
    "        print(\"Previous run is already complete. Starting new run.\")\n",
    "        return run_decoding_with_checkpoints()\n",
    "    \n",
    "    # Resume the previous run\n",
    "    print(f\"Resuming run {all_runs[0]}\")\n",
    "    print(f\"Progress: {completed_tasks}/{total_tasks} tasks completed ({(completed_tasks/total_tasks)*100:.2f}%)\")\n",
    "    \n",
    "    # Set the global RUN_ID and RUN_DIR to match the previous run\n",
    "    global RUN_ID, RUN_DIR\n",
    "    RUN_ID = config['timestamp']\n",
    "    RUN_DIR = most_recent_run\n",
    "    \n",
    "    # Run the decoding with the existing progress\n",
    "    return run_decoding_with_checkpoints()\n",
    "\n",
    "# Function to plot results by group\n",
    "def plot_group_performance(results, config, metric='test_roc_auc', save_path=None):\n",
    "    \"\"\"Plot performance for each concept group across patients\"\"\"\n",
    "    # Extract group pairs\n",
    "    stable_groups = generate_consistent_combinations(config['concepts'], 4)\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    group_labels = []\n",
    "    performance_by_patient = {pid: [] for pid in config['patient_ids']}\n",
    "    errors_by_patient = {pid: [] for pid in config['patient_ids']}\n",
    "    \n",
    "    for group_idx, group_pair in enumerate(stable_groups):\n",
    "        # Create label for this group\n",
    "        group1_str = '+'.join(group_pair[0])\n",
    "        group2_str = '+'.join(group_pair[1])\n",
    "        label = f\"G{group_idx}\"  # Short label for x-axis\n",
    "        group_labels.append(label)\n",
    "        \n",
    "        # Get performance for this group across patients\n",
    "        for pid in config['patient_ids']:\n",
    "            if pid in results and group_idx in results[pid]:\n",
    "                # Collect all results for this patient/group\n",
    "                values = []\n",
    "                for _, result_data in results[pid][group_idx]:\n",
    "                    # Extract the metric value\n",
    "                    result = result_data['result']\n",
    "                    if hasattr(result, metric):\n",
    "                        values.append(getattr(result, metric))\n",
    "                \n",
    "                if values:\n",
    "                    performance_by_patient[pid].append(np.mean(values))\n",
    "                    errors_by_patient[pid].append(np.std(values))\n",
    "                else:\n",
    "                    performance_by_patient[pid].append(np.nan)\n",
    "                    errors_by_patient[pid].append(np.nan)\n",
    "            else:\n",
    "                performance_by_patient[pid].append(np.nan)\n",
    "                errors_by_patient[pid].append(np.nan)\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    bar_width = 0.2\n",
    "    index = np.arange(len(group_labels))\n",
    "    \n",
    "    # Plot bars for each patient\n",
    "    for i, pid in enumerate(config['patient_ids']):\n",
    "        position = index + (i * bar_width)\n",
    "        rects = ax.bar(position, performance_by_patient[pid], \n",
    "                       bar_width, yerr=errors_by_patient[pid], \n",
    "                       label=f\"Patient {pid}\")\n",
    "    \n",
    "    ax.set_xlabel('Concept Groups')\n",
    "    ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "    ax.set_title('Group Decoding Performance Across Patients')\n",
    "    ax.set_xticks(index + bar_width)\n",
    "    ax.set_xticklabels(group_labels)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    \n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7462b1",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7714b413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "969a0aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_decoding_results(results_dir):\n",
    "    \"\"\"\n",
    "    Load all decoding results from a directory and organize them by patient.\n",
    "    \n",
    "    Args:\n",
    "        results_dir: Path to the directory containing result pickle files\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping patient IDs to SingleResultsManager objects\n",
    "    \"\"\"\n",
    "    print(f\"Loading results from {results_dir}...\")\n",
    "    \n",
    "    # Find all result files (excluding config and progress files)\n",
    "    result_files = glob.glob(os.path.join(results_dir, \"patient_*.pkl\"))\n",
    "    print(f\"Found {len(result_files)} result files\")\n",
    "    \n",
    "    if not result_files:\n",
    "        print(\"No result files found!\")\n",
    "        return None\n",
    "    \n",
    "    # Load the configuration file to get metadata\n",
    "    config_file = os.path.join(results_dir, \"config.pkl\")\n",
    "    if os.path.exists(config_file):\n",
    "        config = joblib.load(config_file)\n",
    "        print(f\"Loaded configuration from run {config.get('timestamp', 'unknown')}\")\n",
    "    else:\n",
    "        print(\"No configuration file found, proceeding with limited metadata\")\n",
    "        config = {}\n",
    "    \n",
    "    # Group files by patient\n",
    "    files_by_patient = defaultdict(list)\n",
    "    for filepath in result_files:\n",
    "        # Extract patient ID from filename\n",
    "        filename = os.path.basename(filepath)\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) >= 4 and parts[0] == \"patient\":\n",
    "            pid = parts[1]\n",
    "            files_by_patient[pid].append(filepath)\n",
    "    \n",
    "    print(f\"Found data for {len(files_by_patient)} patients: {', '.join(files_by_patient.keys())}\")\n",
    "    \n",
    "    # Create PatientData objects for each patient\n",
    "    patient_data_objects = {}\n",
    "    for pid in files_by_patient.keys():\n",
    "        try:\n",
    "            print(f\"Creating PatientData object for patient {pid}...\")\n",
    "            patient_data_objects[pid] = PatientData(pid=pid)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating PatientData object for patient {pid}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create SingleResultsManager objects for each patient\n",
    "    managers = {}\n",
    "    \n",
    "    for pid, files in files_by_patient.items():\n",
    "        if pid not in patient_data_objects:\n",
    "            print(f\"Skipping patient {pid} due to missing PatientData object\")\n",
    "            continue\n",
    "        \n",
    "        patient_data = patient_data_objects[pid]\n",
    "        \n",
    "        # Create an empty manager\n",
    "        manager = SingleResultsManager(\n",
    "            patient_data=patient_data,\n",
    "            concept_items=[],  # We'll populate this from the results\n",
    "            epoch='movie',\n",
    "            classifier=LinearSVC(random_state=42),\n",
    "            standardize=True,\n",
    "            pseudo=True  # Match the settings used for original decoding\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Organize results by group\n",
    "        results_by_group = defaultdict(list)\n",
    "        concept_pairs_set = set()  # Keep track of unique concept pairs\n",
    "        \n",
    "        print(f\"Loading {len(files)} result files for patient {pid}...\")\n",
    "        for filepath in tqdm(files):\n",
    "            try:\n",
    "                # Load the result data\n",
    "                result_data = joblib.load(filepath)\n",
    "                \n",
    "                # Extract the group_pair and DecodingResult\n",
    "                group_pair = result_data['group_pair']\n",
    "                result = result_data['result']\n",
    "                \n",
    "                # Add to results_by_group\n",
    "                results_by_group[group_pair].append(result)\n",
    "                concept_pairs_set.add(group_pair)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filepath}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Convert concept_pairs_set to list\n",
    "        concept_pairs = list(concept_pairs_set)\n",
    "        \n",
    "        # Update manager's attributes\n",
    "        manager.concept_items = concept_pairs\n",
    "        manager.results = dict(results_by_group)  # Convert defaultdict to dict\n",
    "        \n",
    "        # Add to the managers dictionary\n",
    "        managers[pid] = manager\n",
    "        \n",
    "        print(f\"Created SingleResultsManager for patient {pid} with {len(concept_pairs)} concept groups\")\n",
    "        \n",
    "    return managers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d910368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading results from ./decoding_res_4x4_april9/run_20250409_153314...\n",
      "Found 1048 result files\n",
      "Loaded configuration from run 20250409_153314\n",
      "Found data for 3 patients: 562, 563, 566\n",
      "Creating PatientData object for patient 562...\n",
      "./Data/40m_act_24_S06E01_30fps_character_frames.csv\n",
      "Creating PatientData object for patient 563...\n",
      "./Data/40m_act_24_S06E01_30fps_character_frames.csv\n",
      "Creating PatientData object for patient 566...\n",
      "./Data/40m_act_24_S06E01_30fps_character_frames.csv\n",
      "Loading 350 result files for patient 562...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:00<00:00, 667.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created SingleResultsManager for patient 562 with 35 concept groups\n",
      "Loading 350 result files for patient 563...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:00<00:00, 611.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created SingleResultsManager for patient 563 with 35 concept groups\n",
      "Loading 348 result files for patient 566...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [00:00<00:00, 672.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created SingleResultsManager for patient 566 with 35 concept groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "RESULTS_DIR = \"./decoding_res_4x4_april9\"\n",
    "run_dir = \"run_20250409_153314\"\n",
    "\n",
    "\n",
    "\n",
    "patient_dict = load_decoding_results(os.path.join(RESULTS_DIR, run_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbfdb76",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47e7b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "p562_manager = patient_dict['562']\n",
    "p563_manager = patient_dict['563']\n",
    "p566_manager = patient_dict['566']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77a542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dichotomy_performance_with_key(results_manager, dichotomy_list, metric='test_roc_auc', figsize=(15, 12), title_suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Plots decoding performance for dichotomies (1-N) and adds a text key below\n",
    "    mapping numbers to the actual group comparisons.\n",
    "\n",
    "    Args:\n",
    "        results_manager: A SingleResultsManager object that has run decoding\n",
    "                         on the items in dichotomy_list.\n",
    "        dichotomy_list: The list of group-vs-group tuples exactly as used\n",
    "                        when running the results_manager. The order determines\n",
    "                        the x-axis order (1-N).\n",
    "        metric (str): The performance metric from DecodingResult to plot.\n",
    "        figsize (tuple): Figure size for the *entire* plot (bars + text key).\n",
    "        title_suffix (str): Optional text to append to the plot title.\n",
    "    \"\"\"\n",
    "    if not results_manager.results:\n",
    "        print(\"Error: results_manager has no results. Did you run run_decoding?\")\n",
    "        return None\n",
    "\n",
    "    expected_num_dichotomies = len(dichotomy_list)\n",
    "    print(f\"Expecting {expected_num_dichotomies} dichotomies based on input list.\")\n",
    "\n",
    "    performance_means = []\n",
    "    performance_stds = []\n",
    "    dichotomy_labels_numeric = [str(i + 1) for i in range(expected_num_dichotomies)]\n",
    "    key_strings = [] # To store formatted strings for the text key\n",
    "\n",
    "    found_count = 0\n",
    "    missing_keys_indices = []\n",
    "\n",
    "    # --- Data Processing ---\n",
    "    for i, dichotomy_key in enumerate(dichotomy_list):\n",
    "        # Format the key string regardless of results being present\n",
    "        group1, group2 = dichotomy_key\n",
    "        group1_str = '+'.join(group1)\n",
    "        group2_str = '+'.join(group2)\n",
    "        key_strings.append(f\"{i+1}: ({group1_str}) vs ({group2_str})\")\n",
    "\n",
    "        if dichotomy_key in results_manager.results:\n",
    "            results_for_key = results_manager.results[dichotomy_key]\n",
    "            if results_for_key:\n",
    "                try:\n",
    "                    values = [getattr(r, metric) for r in results_for_key]\n",
    "                    performance_means.append(np.mean(values))\n",
    "                    performance_stds.append(np.std(values))\n",
    "                    found_count += 1\n",
    "                except AttributeError:\n",
    "                    print(f\"Error: Metric '{metric}' not found for dichotomy {i+1}. Plotting NaN.\")\n",
    "                    performance_means.append(np.nan)\n",
    "                    performance_stds.append(np.nan)\n",
    "                except Exception as e:\n",
    "                     print(f\"Error processing results for dichotomy {i+1}: {e}\")\n",
    "                     performance_means.append(np.nan)\n",
    "                     performance_stds.append(np.nan)\n",
    "            else:\n",
    "                performance_means.append(np.nan)\n",
    "                performance_stds.append(np.nan)\n",
    "                missing_keys_indices.append(i + 1)\n",
    "        else:\n",
    "            performance_means.append(np.nan)\n",
    "            performance_stds.append(np.nan)\n",
    "            missing_keys_indices.append(i + 1)\n",
    "\n",
    "    print(f\"Processed results for {found_count}/{expected_num_dichotomies} dichotomies.\")\n",
    "    if missing_keys_indices:\n",
    "         print(f\"Missing or empty results for dichotomies (numbered 1 to {expected_num_dichotomies}): {sorted(list(set(missing_keys_indices)))}\")\n",
    "\n",
    "\n",
    "    # --- Plotting ---\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    # Define grid: 2 rows, 1 column. Top plot (bars) taller than bottom (text).\n",
    "    # Adjust height_ratios if needed (e.g., [4, 1] for more space for bars)\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1])\n",
    "\n",
    "    # --- Top Subplot: Bar Chart ---\n",
    "    ax_bar = fig.add_subplot(gs[0])\n",
    "\n",
    "    x_positions = np.arange(expected_num_dichotomies)\n",
    "    plot_stds = np.array(performance_stds)\n",
    "    plot_stds[np.isnan(performance_means)] = 0 # Avoid error bars for NaN means\n",
    "\n",
    "    ax_bar.bar(x_positions,\n",
    "               np.nan_to_num(performance_means, nan=0.0),\n",
    "               yerr=plot_stds,\n",
    "               align='center',\n",
    "               alpha=0.75,\n",
    "               ecolor='black',\n",
    "               capsize=4)\n",
    "\n",
    "    ax_bar.set_ylabel(f'{metric.replace(\"_\", \" \").title()}')\n",
    "    # Remove x-label from bar chart, it's implied by the key below\n",
    "    ax_bar.set_xlabel('')\n",
    "    ax_bar.set_xticks(x_positions)\n",
    "    ax_bar.set_xticklabels(dichotomy_labels_numeric, rotation=90, fontsize=8)\n",
    "\n",
    "    if \"roc_auc\" in metric.lower() or \"accuracy\" in metric.lower():\n",
    "         ax_bar.set_ylim(0.0, 1.05)\n",
    "         ax_bar.axhline(0.5, color='grey', linestyle='--', linewidth=0.8, label='Chance (0.5)')\n",
    "         ax_bar.legend(loc='lower right')\n",
    "\n",
    "    ax_bar.grid(axis='y', linestyle=':', linewidth=0.5) # Add horizontal grid lines\n",
    "\n",
    "    # --- Bottom Subplot: Text Key ---\n",
    "    ax_text = fig.add_subplot(gs[1])\n",
    "    ax_text.axis('off') # Hide axes lines and ticks\n",
    "\n",
    "    # Calculate positions for text lines\n",
    "    num_lines = len(key_strings)\n",
    "    # Split into two columns if too many lines\n",
    "    if num_lines > 20: # Adjust this threshold as needed\n",
    "        split_point = (num_lines + 1) // 2\n",
    "        col1_strings = key_strings[:split_point]\n",
    "        col2_strings = key_strings[split_point:]\n",
    "        max_lines_per_col = split_point\n",
    "\n",
    "        # Column 1\n",
    "        y_start = 0.95\n",
    "        y_step = 1.0 / (max_lines_per_col + 1) if max_lines_per_col > 0 else 1.0\n",
    "        for i, line in enumerate(col1_strings):\n",
    "            ax_text.text(0.01, y_start - i * y_step, line,\n",
    "                         ha='left', va='top', fontsize=7, family='monospace')\n",
    "        # Column 2\n",
    "        for i, line in enumerate(col2_strings):\n",
    "             ax_text.text(0.51, y_start - i * y_step, line,\n",
    "                          ha='left', va='top', fontsize=7, family='monospace')\n",
    "\n",
    "    else:\n",
    "        # Single column\n",
    "        y_start = 0.95\n",
    "        y_step = 1.0 / (num_lines + 1) if num_lines > 0 else 1.0\n",
    "        for i, line in enumerate(key_strings):\n",
    "            ax_text.text(0.01, y_start - i * y_step, line,\n",
    "                         ha='left', va='top', fontsize=9, family='monospace')\n",
    "\n",
    "\n",
    "    # --- Overall Figure Title ---\n",
    "    patient_id = results_manager.patient_data.pid\n",
    "    epoch = results_manager.epoch\n",
    "    base_title = f'Group Decoding Performance for {expected_num_dichotomies} Dichotomies'\n",
    "    full_title = f'{base_title}\\nPatient {patient_id}, Epoch: {epoch}'\n",
    "    if title_suffix:\n",
    "        full_title += f\" - {title_suffix}\"\n",
    "    fig.suptitle(full_title, y=0.99) # Adjust y if title overlaps top plot\n",
    "\n",
    "    # Adjust layout - rect might need tuning depending on title length\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "    # Optionally return the processed data\n",
    "    #return {'dichotomy_num': dichotomy_labels_numeric, 'mean_perf': performance_means, 'std_perf': performance_stds, 'key': key_strings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dichotomy_performance_with_key(p562_manager, stable_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf8d492",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dichotomy_performance_with_key(p563_manager, stable_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24433ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dichotomy_performance_with_key(p566_manager, stable_groups)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

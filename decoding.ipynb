{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notebook for decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_structures import PatientData\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Data/40m_act_24_S06E01_30fps_character_frames.csv\n",
      "./Data/40m_act_24_S06E01_30fps_character_frames.csv\n"
     ]
    }
   ],
   "source": [
    "p566 = PatientData(pid='566')\n",
    "p563 = PatientData(pid='563')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptDecoder:\n",
    "    \n",
    "    \"\"\"\n",
    "    Handles decoding for a single concept pair\n",
    "\n",
    "\n",
    "    - add method for PCA visualization in 2D/3D\n",
    "    \"\"\"\n",
    "    def __init__(self, patient_data: PatientData, c1: str, c2: str, epoch: str, classifier: BaseEstimator = LinearSVC()):\n",
    "        self.patient_data = patient_data\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.epoch = epoch\n",
    "        self.classifier = classifier\n",
    "        self.scaler = StandardScaler()\n",
    "        self.metrics = {}\n",
    "    \n",
    "        self.dataset = ConceptPairDataset(\n",
    "            patient_data=self.patient_data,\n",
    "            concept_pair=(self.c1, self.c2),\n",
    "            epoch=self.epoch, \n",
    "            #min_samples = 10\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptPairDataset():\n",
    "    \"\"\"\n",
    "    Class to turn both concept bins - np.ndarrays shape (n_onsets, n_neurons) (each row is a response) into dataset with \n",
    "\n",
    "    2 methods - one with psuedopopulations\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, patient_data: PatientData, concept_pair: Tuple[str, str], \n",
    "                 epoch: str, min_samples: int = 10):\n",
    "        self.patient_data = patient_data\n",
    "        self.c1, self.c2 = concept_pair\n",
    "        self.epoch = epoch\n",
    "        self.min_samples = min_samples\n",
    "\n",
    "    def create_dataset_normal(self, test_size = 0.3):\n",
    "        \"\"\"\n",
    "        Method to create dataset without pseudopops, liable to make unbalanced dataset\n",
    "\n",
    "        Returns X_train, X_test, y_train, y_test\n",
    "        \"\"\"\n",
    "        c1_data, c2_data = self.patient_data.get_concept_data(c1=self.c1, c2=self.c2, epoch=self.epoch)\n",
    "\n",
    "        print(f\"c1 shape: {c1_data.shape[0]}, c2 shape: {c2_data.shape[0]}\")\n",
    "\n",
    "        if len(c1_data) < self.min_samples or len(c2_data) < self.min_samples:\n",
    "            raise ValueError(f\"Insufficient samples for {self.concept1} vs {self.concept2}\")\n",
    "\n",
    "        X = np.vstack([c1_data, c2_data])\n",
    "        y = np.concatenate([np.zeros(len(c1_data)), np.ones(len(c2_data))])\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def create_dataset_pseudo(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing playground\n",
    "\n",
    "\n",
    ":)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [7, 8, 9]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([[4, 5, 6], [7, 8, 9]])\n",
    "x = np.vstack([a, b])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = ConceptPairDataset(p566, ('A.Amar', 'B.Buchanan'), epoch='movie')\n",
    "z1 = ConceptPairDataset(p563, ('A.Amar', 'B.Buchanan'), epoch='movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1 shape: 20, c2 shape: 68\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = z.create_dataset_normal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61, 169)\n",
      "(27, 169)\n",
      "(61,)\n",
      "(27,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

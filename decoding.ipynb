{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notebook for decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ideas: should pseudo data points be proportional to size/amount of real data? form example, pseudo size = c1 + c2 * factor? but this means uneven distribution for smaller concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: test pseudo stuff\n",
    "\n",
    "use dropout!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_structures import PatientData\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p566 = PatientData(pid='566')\n",
    "p563 = PatientData(pid='563')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DecodingResult:\n",
    "    \"\"\"Container for decoding results and metrics\"\"\"\n",
    "    test_accuracy: float\n",
    "    train_accuracy: float\n",
    "    test_roc_auc: float\n",
    "    train_roc_auc: float\n",
    "    train_samples: Dict[str, int]  # Number of samples for each concept in training\n",
    "    test_samples: Dict[str, int]   # Number of samples for each concept in testing\n",
    "    predictions: np.ndarray\n",
    "    true_labels: np.ndarray\n",
    "    classifier: BaseEstimator\n",
    "    data: Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pseudopopulations(\n",
    "    responses: np.ndarray,\n",
    "    n_pseudo: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate random pseudopopulation responses and concatenate with original data.\n",
    "    \n",
    "    Args:\n",
    "        responses: Matrix of shape (n_onsets, n_neurons) containing neural responses\n",
    "        n_pseudo: Number of pseudopopulation responses to generate\n",
    "        random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Array of shape (n_onsets + n_pseudo, n_neurons) containing original responses\n",
    "        concatenated with pseudopopulation responses\n",
    "    \n",
    "    Example:\n",
    "        If responses is shape (5, 10) with 5 onsets and 10 neurons, and n_pseudo=3,\n",
    "        the output will be shape (8, 10) containing the original 5 responses plus\n",
    "        3 new pseudopopulation responses.\n",
    "    \"\"\"\n",
    "    n_onsets, n_neurons = responses.shape\n",
    "    pseudo_responses = np.zeros((n_pseudo, n_neurons))    \n",
    "    for i in range(n_pseudo): # For each pseudopopulation response \n",
    "        for j in range(n_neurons): # For each neuron in psuedoresponse\n",
    "\n",
    "            random_onset = np.random.randint(0, n_onsets) # using random library instead of np in case we seeded random\n",
    "            # Use random onset response for this neuron\n",
    "            pseudo_responses[i, j] = responses[random_onset, j]\n",
    "    combined_responses = np.vstack([responses, pseudo_responses])\n",
    "    \n",
    "    return combined_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptPairDataset():\n",
    "    \"\"\"\n",
    "    Class to turn both concept bins - np.ndarrays shape (n_onsets, n_neurons) (each row is a response) into dataset with \n",
    "\n",
    "    2 methods - one with psuedopopulations\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, patient_data: PatientData, concept_pair: Tuple[str, str], \n",
    "                 epoch: str, min_samples: int = 10):\n",
    "        self.patient_data = patient_data\n",
    "        self.c1, self.c2 = concept_pair\n",
    "        self.epoch = epoch\n",
    "        self.min_samples = min_samples\n",
    "        \n",
    "        # one optimization I could do - currently instantiating new concept pair dataset class every iteration\n",
    "\n",
    "    def create_dataset_normal(self, test_size = 0.3):\n",
    "        \"\"\"\n",
    "        Method to create dataset without pseudopops, liable to make unbalanced dataset\n",
    "\n",
    "        Returns X_train, X_test, y_train, y_test, info: dict\n",
    "        \"\"\"\n",
    "        c1_data, c2_data = self.patient_data.get_concept_data(c1=self.c1, c2=self.c2, epoch=self.epoch)\n",
    "\n",
    "        print(f\"c1 shape: {c1_data.shape[0]}, c2 shape: {c2_data.shape[0]}\")\n",
    "\n",
    "        if len(c1_data) < self.min_samples or len(c2_data) < self.min_samples:\n",
    "            raise ValueError(f\"Insufficient samples for {self.c1} vs {self.c2}\")\n",
    "\n",
    "        X = np.vstack([c1_data, c2_data])\n",
    "        y = np.concatenate([np.zeros(len(c1_data)), np.ones(len(c2_data))])\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "        info = {} #for any extra stuff we wanna pass through\n",
    "        res_dict = {\n",
    "            'X_train': X_train, 'X_test': X_test, 'y_test': y_test, 'y_train': y_train\n",
    "        }\n",
    "\n",
    "        return res_dict, info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def create_dataset_pseudo(self, test_size=0.3, train_size_total=100, test_size_total=50):\n",
    "        \"\"\"\n",
    "        Method to create dataset with pseudopopulations to balance and augment data.\n",
    "        \"\"\"\n",
    "        c1_data, c2_data = self.patient_data.get_concept_data(c1=self.c1, c2=self.c2, epoch=self.epoch)\n",
    "        \n",
    "        print(f\"c1 shape: {c1_data.shape[0]}, c2 shape: {c2_data.shape[0]}\")\n",
    "        \n",
    "        if len(c1_data) < self.min_samples or len(c2_data) < self.min_samples:\n",
    "            raise ValueError(f\"Insufficient samples for {self.c1} vs {self.c2}\")\n",
    "            \n",
    "        # Split real data into train and test sets\n",
    "        c1_train_real, c1_test_real = train_test_split(c1_data, test_size=test_size)\n",
    "        c2_train_real, c2_test_real = train_test_split(c2_data, test_size=test_size)\n",
    "        print(f\"c1 real train size: {c1_train_real.shape[0]}\")\n",
    "        \n",
    "        # Calculate needed pseudo samples\n",
    "        n_pseudo_train_c1 = max(0, train_size_total - len(c1_train_real))\n",
    "        n_pseudo_test_c1 = max(0, test_size_total - len(c1_test_real))\n",
    "        n_pseudo_train_c2 = max(0, train_size_total - len(c2_train_real))\n",
    "        n_pseudo_test_c2 = max(0, test_size_total - len(c2_test_real))\n",
    "        \n",
    "        # Generate pseudopopulations\n",
    "        X_train_c1 = generate_pseudopopulations(c1_train_real, n_pseudo=n_pseudo_train_c1) if n_pseudo_train_c1 > 0 else np.empty((0, c1_train_real.shape[1]))\n",
    "        X_test_c1 = generate_pseudopopulations(c1_test_real, n_pseudo=n_pseudo_test_c1) if n_pseudo_test_c1 > 0 else np.empty((0, c1_test_real.shape[1]))\n",
    "        X_train_c2 = generate_pseudopopulations(c2_train_real, n_pseudo=n_pseudo_train_c2) if n_pseudo_train_c2 > 0 else np.empty((0, c2_train_real.shape[1]))\n",
    "        X_test_c2 = generate_pseudopopulations(c2_test_real, n_pseudo=n_pseudo_test_c2) if n_pseudo_test_c2 > 0 else np.empty((0, c2_test_real.shape[1]))\n",
    "        \n",
    "        \n",
    "        # Create labels (0 for c1, 1 for c2)\n",
    "        y_train = np.concatenate([np.zeros(len(X_train_c1)), np.ones(len(X_train_c2))])\n",
    "        y_test = np.concatenate([np.zeros(len(X_test_c1)), np.ones(len(X_test_c2))])\n",
    "        \n",
    "        # Combine features\n",
    "        X_train = np.vstack([X_train_c1, X_train_c2])\n",
    "        X_test = np.vstack([X_test_c1, X_test_c2])\n",
    "\n",
    "        # Shuffle training data\n",
    "        train_shuffle_idx = np.random.permutation(len(y_train))\n",
    "        X_train = X_train[train_shuffle_idx]\n",
    "        y_train = y_train[train_shuffle_idx]\n",
    "        \n",
    "        # Shuffle test data\n",
    "        test_shuffle_idx = np.random.permutation(len(y_test))\n",
    "        X_test = X_test[test_shuffle_idx]\n",
    "        y_test = y_test[test_shuffle_idx]\n",
    "        \n",
    "        info = {\n",
    "            'test_size_real_data': test_size,\n",
    "            'train_size_total_target': train_size_total,\n",
    "            'test_size_total_target': test_size_total,\n",
    "            'n_pseudo_train_c1': n_pseudo_train_c1,\n",
    "            'n_pseudo_test_c1': n_pseudo_test_c1,\n",
    "            'n_pseudo_train_c2': n_pseudo_train_c2,\n",
    "            'n_pseudo_test_c2': n_pseudo_test_c2,\n",
    "            'n_real_train_c1': len(c1_train_real),\n",
    "            'n_real_test_c1': len(c1_test_real),\n",
    "            'n_real_train_c2': len(c2_train_real),\n",
    "            'n_real_test_c2': len(c2_test_real),\n",
    "        }\n",
    "        \n",
    "        return {'X_train': X_train, 'X_test': X_test, 'y_test': y_test, 'y_train': y_train}, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptDecoder:\n",
    "    \n",
    "    \"\"\"\n",
    "    Handles decoding for a single concept pair\n",
    "\n",
    "    - design choice - will call dataset method in each decoder call?\n",
    "\n",
    "    - problem is that generally we want an object associated with one dataset - this would require an input for pseudo or not\n",
    "    in the class instantiation. however, we want fine control over pseudopops parameters, so this is less good.\n",
    "    one potential solution is a params* dict, but thats complicated. \n",
    "\n",
    "    for consistent stuff\n",
    "\n",
    "    maybe separate classes - instantiate dataset, get training into dict, input training dict into concept decoder?\n",
    "\n",
    "    - add method for PCA visualization in 2D/3D\n",
    "    \"\"\"\n",
    "    def __init__(self, patient_data: PatientData, c1: str, c2: str, epoch: str, classifier: BaseEstimator = LinearSVC(), \n",
    "                 dataset: ConceptPairDataset = None, standardize: bool=False, min_samples=20):\n",
    "        self.patient_data = patient_data\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.epoch = epoch\n",
    "        self.classifier = classifier\n",
    "        self.min_samples=min_samples\n",
    "\n",
    "        self.scaler = StandardScaler() if standardize else None\n",
    "        self.metrics = {}\n",
    "        self.enough_data = True # for efficiency\n",
    "\n",
    "    \n",
    "        if not dataset:\n",
    "            self.dataset = ConceptPairDataset( #type: ignore\n",
    "                patient_data=self.patient_data,\n",
    "                concept_pair=(self.c1, self.c2),\n",
    "                epoch=self.epoch, \n",
    "                min_samples = self.min_samples \n",
    "            )\n",
    "        else:\n",
    "            self.dataset = dataset\n",
    "\n",
    "        try:\n",
    "            _, _ = self.dataset.create_dataset_normal()\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping concept pair {self.c1}, {self.c2}: {e}\") # Inform user of skipped pair and reason\n",
    "            self.enough_data = False\n",
    "        \n",
    "    def decode_normal(self, test_size=0.3):\n",
    "        try:\n",
    "            data_dict, info = self.dataset.create_dataset_normal(test_size=test_size)\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping concept pair {self.c1}, {self.c2}: {e}\") # Inform user of skipped pair and reason\n",
    "            return None # Return None to indicate decoding failure for this pair\n",
    "        return self._decode(data_dict=data_dict)\n",
    "\n",
    "\n",
    "    def decode_pseudo(self, train_size_total=300, test_size_total=100, test_size=0.3):\n",
    "        try:\n",
    "            data_dict, info = self.dataset.create_dataset_pseudo(test_size=test_size, train_size_total=train_size_total, test_size_total=test_size_total)\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping concept pair {self.c1}, {self.c2}: {e}\") # Inform user of skipped pair and reason\n",
    "            return None # Return None to indicate decoding failure for this pair\n",
    "        return self._decode(data_dict=data_dict)\n",
    "\n",
    "\n",
    "\n",
    "    def _decode(self, data_dict) -> DecodingResult: \n",
    "        \"\"\"\n",
    "        Performs decoding on the concept pair using normal dataset\n",
    "        \n",
    "        Args:\n",
    "            test_size: Fraction of data to use for testing\n",
    "            \n",
    "        Returns:\n",
    "            DecodingResult containing metrics and predictions\n",
    "        \"\"\"\n",
    "\n",
    "        X_train = data_dict['X_train']\n",
    "        X_test = data_dict['X_test']\n",
    "        y_train = data_dict['y_train']\n",
    "        y_test = data_dict['y_test']\n",
    "        print(f\"split sizes: X_train: {X_train.shape}, X_test: {X_test.shape}, y_train: {y_train.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "\n",
    "        if self.scaler:\n",
    "            X_train = self.scaler.fit_transform(X_train)\n",
    "            X_test = self.scaler.transform(X_test)\n",
    "\n",
    "        self.classifier.fit(X_train, y_train)\n",
    "        \n",
    "        # Get predictions\n",
    "        y_train_pred = self.classifier.predict(X_train)\n",
    "        y_pred = self.classifier.predict(X_test)\n",
    "\n",
    "        # Calculate metrics for train and test\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        test_accuracy = accuracy_score(y_test, y_pred)\n",
    "        train_roc_auc = roc_auc_score(y_train, y_train_pred) # or use decision_function for prob based ROC AUC if needed\n",
    "        test_roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "        \n",
    "        train_samples = {\n",
    "            self.c1: np.sum(y_train == 0),\n",
    "            self.c2: np.sum(y_train == 1)\n",
    "        }\n",
    "        test_samples = {\n",
    "            self.c1: np.sum(y_test == 0),\n",
    "            self.c2: np.sum(y_test == 1)\n",
    "        }\n",
    "\n",
    "        return DecodingResult(\n",
    "            train_accuracy=train_accuracy,\n",
    "            train_roc_auc=train_roc_auc,\n",
    "            test_accuracy=test_accuracy,\n",
    "            test_roc_auc=test_roc_auc,\n",
    "            train_samples=train_samples,\n",
    "            test_samples=test_samples,\n",
    "            predictions=y_pred,\n",
    "            true_labels=y_test,\n",
    "            classifier=self.classifier,\n",
    "            data=data_dict\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodingResultsManager:\n",
    "    \"\"\"\n",
    "    Manages decoding results for multiple concept pairs for a single patient and epoch.\n",
    "    \"\"\"\n",
    "    def __init__(self, patient_data: PatientData, concept_pairs: List[Tuple[str, str]], \n",
    "                 epoch: str, classifier: BaseEstimator = LinearSVC(), \n",
    "                 standardize: bool = False, pseudo=False, **pseudo_kwargs):\n",
    "        self.patient_data = patient_data\n",
    "        self.concept_pairs = concept_pairs\n",
    "        self.epoch = epoch\n",
    "        self.classifier = classifier # Default classifier for all decoders, can be overridden\n",
    "        self.standardize = standardize # Default standardization for all decoders\n",
    "        self.results: Dict[List[Tuple[str, str]], DecodingResult] = {} # Store results here, key is concept pair\n",
    "        self.pseudo = pseudo\n",
    "        self.pseudo_params = pseudo_kwargs\n",
    "\n",
    "    def run_decoding_for_pairs(self, num_iter: int = 1) -> None:\n",
    "        \"\"\"\n",
    "        Runs decoding for all concept pairs provided in the constructor.\n",
    "        Stores the DecodingResult in the self.results dictionary.\n",
    "        \"\"\"\n",
    "        self.results = {} # reset results every time\n",
    "        for c1, c2 in self.concept_pairs:\n",
    "            decoder = ConceptDecoder(\n",
    "                    patient_data=self.patient_data,\n",
    "                    c1=c1,\n",
    "                    c2=c2,\n",
    "                    epoch=self.epoch,\n",
    "                    classifier=self.classifier,\n",
    "                    standardize=self.standardize\n",
    "                )\n",
    "            if decoder.enough_data: # efficient way of not rechecking for sufficient samples\n",
    "                for i in range(num_iter):\n",
    "                    print(f\"concept decoding: {c1} vs {c2}, iteration #{i}\")\n",
    "                    if self.pseudo:\n",
    "                        if self.pseudo_params:\n",
    "                            result = decoder.decode_pseudo(**self.pseudo_params)\n",
    "                        else:\n",
    "                            result = decoder.decode_pseudo()\n",
    "                    else:\n",
    "                        result = decoder.decode()\n",
    "                    if result is not None: # Only store if decode was successful (not None)\n",
    "                        if (c1, c2) not in self.results:\n",
    "                            self.results[(c1, c2)] = [result]\n",
    "                        else:\n",
    "                                self.results[(c1, c2)].append(result)\n",
    "\n",
    "\n",
    "    def plot_train_test_performance_heatmap(self, metric='test_roc_auc', figsize=(20, 10)):\n",
    "        \"\"\"\n",
    "        Generates and displays a combined heatmap of training and testing performance for all concept pairs.\n",
    "        For multiple iterations, shows mean performance with standard deviation in parentheses.\n",
    "        \n",
    "        Args:\n",
    "            metric (str): One of 'test_accuracy', 'train_accuracy', 'test_roc_auc', 'train_roc_auc'\n",
    "            figsize (tuple): Figure size for the plot\n",
    "        \"\"\"\n",
    "        concepts = sorted(list(set([c for pair in self.concept_pairs for c in pair])))\n",
    "        n_concepts = len(concepts)\n",
    "        \n",
    "        # Initialize matrices for means and standard deviations\n",
    "        train_mean_matrix = np.full((n_concepts, n_concepts), np.nan)\n",
    "        test_mean_matrix = np.full((n_concepts, n_concepts), np.nan)\n",
    "        train_std_matrix = np.full((n_concepts, n_concepts), np.nan)\n",
    "        test_std_matrix = np.full((n_concepts, n_concepts), np.nan)\n",
    "\n",
    "        concept_to_idx = {concept: i for i, concept in enumerate(concepts)}\n",
    "\n",
    "        for concept_pair, results in self.results.items():\n",
    "            if results:  # Check if results exist for this pair\n",
    "                c1, c2 = concept_pair\n",
    "                i, j = concept_to_idx[c1], concept_to_idx[c2]\n",
    "                \n",
    "                # Extract values for all iterations\n",
    "                if 'roc_auc' in metric:\n",
    "                    train_values = [r.train_roc_auc for r in results]\n",
    "                    test_values = [r.test_roc_auc for r in results]\n",
    "                else:  # accuracy\n",
    "                    train_values = [r.train_accuracy for r in results]\n",
    "                    test_values = [r.test_accuracy for r in results]\n",
    "                \n",
    "                # Calculate mean and std\n",
    "                train_mean = np.mean(train_values)\n",
    "                test_mean = np.mean(test_values)\n",
    "                train_std = np.std(train_values)\n",
    "                test_std = np.std(test_values)\n",
    "                \n",
    "                # Fill matrices symmetrically\n",
    "                for matrix, value in [(train_mean_matrix, train_mean), \n",
    "                                    (test_mean_matrix, test_mean),\n",
    "                                    (train_std_matrix, train_std),\n",
    "                                    (test_std_matrix, test_std)]:\n",
    "                    matrix[i, j] = value\n",
    "                    matrix[j, i] = value\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        def annotate_heatmap(ax, mean_matrix, std_matrix):\n",
    "            \"\"\"Helper function to annotate heatmap with mean ± std\"\"\"\n",
    "            for i in range(mean_matrix.shape[0]):\n",
    "                for j in range(mean_matrix.shape[1]):\n",
    "                    if not np.isnan(mean_matrix[i, j]):\n",
    "                        text = f'{mean_matrix[i, j]:.3f}\\n(±{std_matrix[i, j]:.3f})'\n",
    "                        ax.text(j + 0.5, i + 0.5, text,\n",
    "                            ha='center', va='center',\n",
    "                            color='white' if mean_matrix[i, j] > 0.5 else 'black',\n",
    "                            fontsize=8)\n",
    "                    else:\n",
    "                        ax.text(j + 0.5, i + 0.5, 'N/A',\n",
    "                            ha='center', va='center',\n",
    "                            color='gray')\n",
    "\n",
    "        # Plot heatmaps\n",
    "        for ax, mean_matrix, std_matrix, title in [\n",
    "            (ax1, train_mean_matrix, train_std_matrix, 'Training'),\n",
    "            (ax2, test_mean_matrix, test_std_matrix, 'Test')\n",
    "        ]:\n",
    "            sns.heatmap(mean_matrix, ax=ax,\n",
    "                    xticklabels=concepts,\n",
    "                    yticklabels=concepts,\n",
    "                    cmap='viridis',\n",
    "                    vmin=0.0,\n",
    "                    vmax=1.0,\n",
    "                    center=0.4,\n",
    "                    annot=False)  # We'll add custom annotations\n",
    "            \n",
    "            annotate_heatmap(ax, mean_matrix, std_matrix)\n",
    "            \n",
    "            ax.set_title(f'{title} {metric.replace(\"test_\", \"\").replace(\"_\", \" \").title()}')\n",
    "            ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "        plt.suptitle(\n",
    "            f'Train vs Test Performance for Concept Decoding\\nPatient {self.patient_data.pid}, Epoch: {self.epoch}\\n(mean ± std across {len(next(iter(self.results.values())))} iterations)',\n",
    "            y=1.05\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Return statistics for analysis if needed\n",
    "        stats = {\n",
    "            'train_mean': train_mean_matrix,\n",
    "            'train_std': train_std_matrix,\n",
    "            'test_mean': test_mean_matrix,\n",
    "            'test_std': test_std_matrix\n",
    "        }\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All concept decoding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing playground\n",
    "\n",
    "\n",
    ":)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_pairs_to_decode = []\n",
    "concept_list = list(p566.movie_df.columns)\n",
    "#remove non-concept columns if they exist\n",
    "concept_list = [col for col in concept_list if col not in ['Frame', 'time_sec', 'rel_corrected_time_sec', 'uncorrected_time_sec']]\n",
    "\n",
    "\n",
    "# doing to 5 for time\n",
    "\n",
    "for i, concept1 in enumerate(concept_list[:5]):\n",
    "    for concept2 in concept_list[i+1:5]: #avoid duplicates and self-pairs\n",
    "        concept_pairs_to_decode.append((concept1, concept2))\n",
    "\n",
    "print(f\"Number of concept pairs to decode: {len(concept_pairs_to_decode)}\")\n",
    "print(concept_pairs_to_decode[:5]) # Print first 5 pairs as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = DecodingResultsManager(\n",
    "    patient_data=p566,\n",
    "    concept_pairs=concept_pairs_to_decode,\n",
    "    epoch='movie',\n",
    "    standardize=False,\n",
    "    pseudo=True\n",
    "    # no kwargs -> default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.run_decoding_for_pairs(num_iter=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.plot_train_test_performance_heatmap(metric='test_roc_auc') # Example ROC AUC heatmap\n",
    "manager.plot_train_test_performance_heatmap(metric='test_accuracy') # Example Accuracy heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check for multiple decode method calls being different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = ConceptDecoder(\n",
    "                    patient_data=p563,\n",
    "                    c1='A.Fayed',\n",
    "                    c2='B.Buchanan',\n",
    "                    epoch='movie',\n",
    "                    classifier=LinearSVC,\n",
    "                    standardize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ConceptPairDataset(\n",
    "    patient_data=p563,\n",
    "    concept_pair=('A.Fayed', 'B.Buchanan'),  # example concepts from your code\n",
    "    epoch='movie',\n",
    "    min_samples=10\n",
    ")\n",
    "\n",
    "# Test both dataset creation methods\n",
    "print(\"\\nTesting normal dataset creation:\")\n",
    "normal_data, normal_info = test_dataset.create_dataset_normal(test_size=0.3)\n",
    "\n",
    "# Print shapes and class distribution\n",
    "print(\"\\nNormal dataset shapes:\")\n",
    "print(f\"X_train shape: {normal_data['X_train'].shape}\")\n",
    "print(f\"X_test shape: {normal_data['X_test'].shape}\")\n",
    "print(f\"y_train shape: {normal_data['y_train'].shape}\")\n",
    "print(f\"y_test shape: {normal_data['y_test'].shape}\")\n",
    "\n",
    "print(\"\\nClass distribution in normal dataset:\")\n",
    "print(f\"Training class 0: {sum(normal_data['y_train'] == 0)}\")\n",
    "print(f\"Training class 1: {sum(normal_data['y_train'] == 1)}\")\n",
    "print(f\"Test class 0: {sum(normal_data['y_test'] == 0)}\")\n",
    "print(f\"Test class 1: {sum(normal_data['y_test'] == 1)}\")\n",
    "\n",
    "print(\"\\nTesting pseudo dataset creation:\")\n",
    "pseudo_data, pseudo_info = test_dataset.create_dataset_pseudo(\n",
    "    test_size=0.3,\n",
    "    train_size_total=100,  # target 100 samples per class in training\n",
    "    test_size_total=50     # target 50 samples per class in testing\n",
    ")\n",
    "\n",
    "print(\"\\nPseudo dataset shapes:\")\n",
    "print(f\"X_train shape: {pseudo_data['X_train'].shape}\")\n",
    "print(f\"X_test shape: {pseudo_data['X_test'].shape}\")\n",
    "print(f\"y_train shape: {pseudo_data['y_train'].shape}\")\n",
    "print(f\"y_test shape: {pseudo_data['y_test'].shape}\")\n",
    "\n",
    "print(\"\\nClass distribution in pseudo dataset:\")\n",
    "print(f\"Training class 0: {sum(pseudo_data['y_train'] == 0)}\")\n",
    "print(f\"Training class 1: {sum(pseudo_data['y_train'] == 1)}\")\n",
    "print(f\"Test class 0: {sum(pseudo_data['y_test'] == 0)}\")\n",
    "print(f\"Test class 1: {sum(pseudo_data['y_test'] == 1)}\")\n",
    "\n",
    "# Check if data is shuffled\n",
    "print(\"\\nChecking if data is shuffled (should see mixed 0s and 1s):\")\n",
    "print(\"First 10 training labels:\", pseudo_data['y_train'][:10])\n",
    "print(\"Last 10 training labels:\", pseudo_data['y_train'][-10:])\n",
    "\n",
    "# Print pseudo dataset info\n",
    "print(\"\\nPseudo dataset generation info:\")\n",
    "for key, value in pseudo_info.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1, _ = test_dataset.create_dataset_pseudo()\n",
    "r2, _ = test_dataset.create_dataset_pseudo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(r1['X_test'] == r2['X_test']).all()\n",
    "(r1['X_train'] == r2['X_train']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1['X_test'][:5, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1['X_test'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2['X_test'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

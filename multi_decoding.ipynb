{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function file\n",
    "from data_structures import PatientData\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from decoders import generate_pseudopopulations, DecodingResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiPatientDataset:\n",
    "    \"\"\"\n",
    "    Class to combine concept data from multiple patients for decoding.\n",
    "    Similar to ConceptPairDataset but works across multiple patients.\n",
    "    \"\"\"\n",
    "    def __init__(self, patient_data_list: List[PatientData], concept_pair: Tuple[str, str], \n",
    "                epoch: str, min_samples: int = 10, neurons_list=None):\n",
    "        self.patient_data_list = patient_data_list\n",
    "        self.c1, self.c2 = concept_pair\n",
    "        self.epoch = epoch\n",
    "        self.min_samples = min_samples\n",
    "        \n",
    "        # If neurons_list is not provided, use all neurons from each patient\n",
    "        if neurons_list is None:\n",
    "            self.neurons_list = [p.neurons for p in patient_data_list]\n",
    "        else:\n",
    "            self.neurons_list = neurons_list\n",
    "            \n",
    "    def create_dataset_normal(self, test_size=0.3):\n",
    "        \"\"\"\n",
    "        Create a combined dataset from multiple patients without pseudopopulations.\n",
    "        \"\"\"\n",
    "        all_c1_data = []\n",
    "        all_c2_data = []\n",
    "        \n",
    "        # Collect data from each patient\n",
    "        for patient_data, neurons in zip(self.patient_data_list, self.neurons_list):\n",
    "            try:\n",
    "                c1_data, c2_data = patient_data.get_concept_data(\n",
    "                    c1=self.c1, c2=self.c2, epoch=self.epoch, neurons=neurons\n",
    "                )\n",
    "                all_c1_data.append(c1_data)\n",
    "                all_c2_data.append(c2_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping patient {patient_data.pid} for {self.c1} vs {self.c2}: {e}\")\n",
    "                \n",
    "        # Ensure we have data from at least one patient\n",
    "        if not all_c1_data or not all_c2_data:\n",
    "            raise ValueError(f\"No valid data for {self.c1} vs {self.c2} across patients\")\n",
    "        \n",
    "        # Combine data across patients (along neuron dimension)\n",
    "        # Each onset is paired with all neurons from all patients\n",
    "        combined_c1_data = np.hstack([c1_data for c1_data in all_c1_data])\n",
    "        combined_c2_data = np.hstack([c2_data for c2_data in all_c2_data])\n",
    "        \n",
    "        if len(combined_c1_data) < self.min_samples or len(combined_c2_data) < self.min_samples:\n",
    "            raise ValueError(f\"Insufficient samples for {self.c1} vs {self.c2}\")\n",
    "            \n",
    "        X = np.vstack([combined_c1_data, combined_c2_data])\n",
    "        y = np.concatenate([np.zeros(len(combined_c1_data)), np.ones(len(combined_c2_data))])\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "        \n",
    "        info = {\n",
    "            'n_patients': len(self.patient_data_list),\n",
    "            'patient_ids': [p.pid for p in self.patient_data_list],\n",
    "            'total_neurons': sum(len(neurons) for neurons in self.neurons_list)\n",
    "        }\n",
    "        \n",
    "        res_dict = {\n",
    "            'X_train': X_train, 'X_test': X_test, 'y_test': y_test, 'y_train': y_train\n",
    "        }\n",
    "        \n",
    "        return res_dict, info\n",
    "        \n",
    "    def create_dataset_pseudo(self, test_size=0.3, train_size_total=100, test_size_total=50):\n",
    "        \"\"\"\n",
    "        Create a combined dataset from multiple patients with pseudopopulations.\n",
    "        \"\"\"\n",
    "        # Similar implementation to create_dataset_normal, but with pseudopopulation generation\n",
    "        # First collect all data\n",
    "        all_c1_data = []\n",
    "        all_c2_data = []\n",
    "        \n",
    "        for patient_data, neurons in zip(self.patient_data_list, self.neurons_list):\n",
    "            try:\n",
    "                c1_data, c2_data = patient_data.get_concept_data(\n",
    "                    c1=self.c1, c2=self.c2, epoch=self.epoch, neurons=neurons\n",
    "                )\n",
    "                all_c1_data.append(c1_data)\n",
    "                all_c2_data.append(c2_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping patient {patient_data.pid} for {self.c1} vs {self.c2}: {e}\")\n",
    "        \n",
    "        # Ensure we have data\n",
    "        if not all_c1_data or not all_c2_data:\n",
    "            raise ValueError(f\"No valid data for {self.c1} vs {self.c2} across patients\")\n",
    "            \n",
    "        # Combine data across patients\n",
    "        combined_c1_data = np.hstack([c1_data for c1_data in all_c1_data])\n",
    "        combined_c2_data = np.hstack([c2_data for c2_data in all_c2_data])\n",
    "        \n",
    "        # Split into train/test\n",
    "        c1_train_real, c1_test_real = train_test_split(combined_c1_data, test_size=test_size)\n",
    "        c2_train_real, c2_test_real = train_test_split(combined_c2_data, test_size=test_size)\n",
    "        \n",
    "        # Calculate needed pseudo samples\n",
    "        n_pseudo_train_c1 = max(0, train_size_total - len(c1_train_real))\n",
    "        n_pseudo_test_c1 = max(0, test_size_total - len(c1_test_real))\n",
    "        n_pseudo_train_c2 = max(0, train_size_total - len(c2_train_real))\n",
    "        n_pseudo_test_c2 = max(0, test_size_total - len(c2_test_real))\n",
    "        \n",
    "        # Generate pseudopopulations using your existing function\n",
    "        X_train_c1 = generate_pseudopopulations(c1_train_real, n_pseudo=n_pseudo_train_c1) if n_pseudo_train_c1 > 0 else c1_train_real\n",
    "        X_test_c1 = generate_pseudopopulations(c1_test_real, n_pseudo=n_pseudo_test_c1) if n_pseudo_test_c1 > 0 else c1_test_real\n",
    "        X_train_c2 = generate_pseudopopulations(c2_train_real, n_pseudo=n_pseudo_train_c2) if n_pseudo_train_c2 > 0 else c2_train_real\n",
    "        X_test_c2 = generate_pseudopopulations(c2_test_real, n_pseudo=n_pseudo_test_c2) if n_pseudo_test_c2 > 0 else c2_test_real\n",
    "        \n",
    "        # Create labels and combine\n",
    "        y_train = np.concatenate([np.zeros(len(X_train_c1)), np.ones(len(X_train_c2))])\n",
    "        y_test = np.concatenate([np.zeros(len(X_test_c1)), np.ones(len(X_test_c2))])\n",
    "        \n",
    "        X_train = np.vstack([X_train_c1, X_train_c2])\n",
    "        X_test = np.vstack([X_test_c1, X_test_c2])\n",
    "        \n",
    "        # Shuffle\n",
    "        train_shuffle_idx = np.random.permutation(len(y_train))\n",
    "        X_train = X_train[train_shuffle_idx]\n",
    "        y_train = y_train[train_shuffle_idx]\n",
    "        \n",
    "        test_shuffle_idx = np.random.permutation(len(y_test))\n",
    "        X_test = X_test[test_shuffle_idx]\n",
    "        y_test = y_test[test_shuffle_idx]\n",
    "        \n",
    "        info = {\n",
    "            'n_patients': len(self.patient_data_list),\n",
    "            'patient_ids': [p.pid for p in self.patient_data_list],\n",
    "            'total_neurons': sum(len(neurons) for neurons in self.neurons_list),\n",
    "            'n_pseudo_train_c1': n_pseudo_train_c1,\n",
    "            'n_pseudo_test_c1': n_pseudo_test_c1,\n",
    "            'n_pseudo_train_c2': n_pseudo_train_c2,\n",
    "            'n_pseudo_test_c2': n_pseudo_test_c2,\n",
    "            'n_real_train_c1': len(c1_train_real),\n",
    "            'n_real_test_c1': len(c1_test_real),\n",
    "            'n_real_train_c2': len(c2_train_real),\n",
    "            'n_real_test_c2': len(c2_test_real),\n",
    "        }\n",
    "        \n",
    "        return {'X_train': X_train, 'X_test': X_test, 'y_test': y_test, 'y_train': y_train}, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiPatientDecoder:\n",
    "    \"\"\"Handles decoding across multiple patients for a single concept pair\"\"\"\n",
    "    \n",
    "    def __init__(self, patient_data_list: List[PatientData], c1: str, c2: str, epoch: str, \n",
    "                 classifier: BaseEstimator = LinearSVC(), standardize: bool=False, \n",
    "                 min_samples=20, neurons_list=None):\n",
    "        self.patient_data_list = patient_data_list\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.epoch = epoch\n",
    "        self.classifier = classifier\n",
    "        self.min_samples = min_samples\n",
    "        self.neurons_list = neurons_list\n",
    "        \n",
    "        self.scaler = StandardScaler() if standardize else None\n",
    "        self.enough_data = True\n",
    "        \n",
    "        self.dataset = MultiPatientDataset( # add option to input dataset? not high priority\n",
    "            patient_data_list=self.patient_data_list,\n",
    "            concept_pair=(self.c1, self.c2),\n",
    "            epoch=self.epoch,\n",
    "            min_samples=self.min_samples,\n",
    "            neurons_list=self.neurons_list\n",
    "        )\n",
    "        \n",
    "        # Check if we have enough data\n",
    "        try:\n",
    "            _, _ = self.dataset.create_dataset_normal()\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping concept pair {self.c1}, {self.c2}: {e}\")\n",
    "            self.enough_data = False\n",
    "            \n",
    "    def decode_normal(self, test_size=0.3):\n",
    "        try:\n",
    "            data_dict, info = self.dataset.create_dataset_normal(test_size=test_size)\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping concept pair {self.c1}, {self.c2}: {e}\")\n",
    "            return None\n",
    "        return self._decode(data_dict=data_dict)\n",
    "        \n",
    "    def decode_pseudo(self, train_size_total=200, test_size_total=67, test_size=0.3):\n",
    "        try:\n",
    "            data_dict, info = self.dataset.create_dataset_pseudo(\n",
    "                test_size=test_size, \n",
    "                train_size_total=train_size_total, \n",
    "                test_size_total=test_size_total\n",
    "            )\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping concept pair {self.c1}, {self.c2}: {e}\")\n",
    "            return None\n",
    "        return self._decode(data_dict=data_dict)\n",
    "        \n",
    "    def _decode(self, data_dict) -> DecodingResult:\n",
    "        \"\"\"Same as ConceptDecoder._decode\"\"\"\n",
    "        X_train = data_dict['X_train']\n",
    "        X_test = data_dict['X_test']\n",
    "        y_train = data_dict['y_train']\n",
    "        y_test = data_dict['y_test']\n",
    "\n",
    "        if self.scaler:\n",
    "            X_train = self.scaler.fit_transform(X_train)\n",
    "            X_test = self.scaler.transform(X_test)\n",
    "\n",
    "        self.classifier.fit(X_train, y_train)\n",
    "        \n",
    "        # Get predictions\n",
    "        y_train_pred = self.classifier.predict(X_train)\n",
    "        y_pred = self.classifier.predict(X_test)\n",
    "\n",
    "        # Calculate metrics for train and test\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        test_accuracy = accuracy_score(y_test, y_pred)\n",
    "        train_roc_auc = roc_auc_score(y_train, y_train_pred)\n",
    "        test_roc_auc = roc_auc_score(y_test, y_pred)\n",
    "        \n",
    "        train_samples = {\n",
    "            self.c1: np.sum(y_train == 0),\n",
    "            self.c2: np.sum(y_train == 1)\n",
    "        }\n",
    "        test_samples = {\n",
    "            self.c1: np.sum(y_test == 0),\n",
    "            self.c2: np.sum(y_test == 1)\n",
    "        }\n",
    "\n",
    "        return DecodingResult(\n",
    "            train_accuracy=train_accuracy,\n",
    "            train_roc_auc=train_roc_auc,\n",
    "            test_accuracy=test_accuracy,\n",
    "            test_roc_auc=test_roc_auc,\n",
    "            train_samples=train_samples,\n",
    "            test_samples=test_samples,\n",
    "            predictions=y_pred,\n",
    "            true_labels=y_test,\n",
    "            classifier=self.classifier,\n",
    "            data=data_dict\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiResultsManager:\n",
    "    \"\"\"\n",
    "    Manages decoding results for multiple patients and multiple concept pairs.\n",
    "    Similar to SingleResultsManager but for multiple patients.\n",
    "    \"\"\"\n",
    "    def __init__(self, patient_data_list: List[PatientData], concept_pairs: List[Tuple[str, str]], \n",
    "                 epoch: str, classifier: BaseEstimator = LinearSVC(), \n",
    "                 standardize: bool = False, pseudo=False, neurons_list=None, **pseudo_kwargs):\n",
    "        self.patient_data_list = patient_data_list\n",
    "        self.concept_pairs = concept_pairs\n",
    "        self.epoch = epoch\n",
    "        self.classifier = classifier\n",
    "        self.standardize = standardize\n",
    "        self.results = {}\n",
    "        self.pseudo = pseudo\n",
    "        self.pseudo_params = pseudo_kwargs\n",
    "        self.neurons_list = neurons_list\n",
    "        \n",
    "    def run_decoding_for_pairs(self, num_iter: int = 1) -> None:\n",
    "        \"\"\"\n",
    "        Runs decoding for all concept pairs provided in the constructor.\n",
    "        \"\"\"\n",
    "        self.results = {}\n",
    "        for c1, c2 in tqdm(self.concept_pairs, desc=f\"Decoding for {[p.pid for p in self.patient_data_list]}\"):\n",
    "            decoder = MultiPatientDecoder(\n",
    "                patient_data_list=self.patient_data_list,\n",
    "                c1=c1,\n",
    "                c2=c2,\n",
    "                epoch=self.epoch,\n",
    "                classifier=self.classifier,\n",
    "                standardize=self.standardize,\n",
    "                neurons_list=self.neurons_list\n",
    "            )\n",
    "            \n",
    "            if decoder.enough_data:\n",
    "                for i in range(num_iter):\n",
    "                    if self.pseudo:\n",
    "                        if self.pseudo_params:\n",
    "                            result = decoder.decode_pseudo(**self.pseudo_params)\n",
    "                        else:\n",
    "                            result = decoder.decode_pseudo()\n",
    "                    else:\n",
    "                        result = decoder.decode_normal()\n",
    "                        \n",
    "                    if result is not None:\n",
    "                        if (c1, c2) not in self.results:\n",
    "                            self.results[(c1, c2)] = [result]\n",
    "                        else:\n",
    "                            self.results[(c1, c2)].append(result)\n",
    "                            \n",
    "    # Use the same plot_train_test_performance_heatmap method as in SingleResultsManager\n",
    "    def plot_train_test_performance_heatmap(self, metric='test_roc_auc', figsize=(20, 10)):\n",
    "        \"\"\"\n",
    "        Generates and displays a combined heatmap of training and testing performance for all concept pairs.\n",
    "        For multiple iterations, shows mean performance with standard deviation in parentheses.\n",
    "        \n",
    "        Args:\n",
    "            metric (str): One of 'test_accuracy', 'train_accuracy', 'test_roc_auc', 'train_roc_auc'\n",
    "            figsize (tuple): Figure size for the plot\n",
    "        \"\"\"\n",
    "        concepts = sorted(list(set([c for pair in self.concept_pairs for c in pair])))\n",
    "        n_concepts = len(concepts)\n",
    "        \n",
    "        # Initialize matrices for means and standard deviations\n",
    "        train_mean_matrix = np.full((n_concepts, n_concepts), np.nan)\n",
    "        test_mean_matrix = np.full((n_concepts, n_concepts), np.nan)\n",
    "        train_std_matrix = np.full((n_concepts, n_concepts), np.nan)\n",
    "        test_std_matrix = np.full((n_concepts, n_concepts), np.nan)\n",
    "\n",
    "        concept_to_idx = {concept: i for i, concept in enumerate(concepts)}\n",
    "\n",
    "        for concept_pair, results in self.results.items():\n",
    "            if results:  # Check if results exist for this pair\n",
    "                c1, c2 = concept_pair\n",
    "                i, j = concept_to_idx[c1], concept_to_idx[c2]\n",
    "                \n",
    "                # Extract values for all iterations\n",
    "                if 'roc_auc' in metric:\n",
    "                    train_values = [r.train_roc_auc for r in results]\n",
    "                    test_values = [r.test_roc_auc for r in results]\n",
    "                else:  # accuracy\n",
    "                    train_values = [r.train_accuracy for r in results]\n",
    "                    test_values = [r.test_accuracy for r in results]\n",
    "                \n",
    "                # Calculate mean and std\n",
    "                train_mean = np.mean(train_values)\n",
    "                test_mean = np.mean(test_values)\n",
    "                train_std = np.std(train_values)\n",
    "                test_std = np.std(test_values)\n",
    "                \n",
    "                # Fill matrices symmetrically\n",
    "                for matrix, value in [(train_mean_matrix, train_mean), \n",
    "                                    (test_mean_matrix, test_mean),\n",
    "                                    (train_std_matrix, train_std),\n",
    "                                    (test_std_matrix, test_std)]:\n",
    "                    matrix[i, j] = value\n",
    "                    matrix[j, i] = value\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        def annotate_heatmap(ax, mean_matrix, std_matrix):\n",
    "            \"\"\"Helper function to annotate heatmap with mean ± std\"\"\"\n",
    "            for i in range(mean_matrix.shape[0]):\n",
    "                for j in range(mean_matrix.shape[1]):\n",
    "                    if not np.isnan(mean_matrix[i, j]):\n",
    "                        text = f'{mean_matrix[i, j]:.3f}\\n(±{std_matrix[i, j]:.3f})'\n",
    "                        ax.text(j + 0.5, i + 0.5, text,\n",
    "                            ha='center', va='center',\n",
    "                            color='white' if mean_matrix[i, j] > 0.5 else 'black',\n",
    "                            fontsize=8)\n",
    "                    else:\n",
    "                        ax.text(j + 0.5, i + 0.5, 'N/A',\n",
    "                            ha='center', va='center',\n",
    "                            color='gray')\n",
    "\n",
    "        # Plot heatmaps\n",
    "        for ax, mean_matrix, std_matrix, title in [\n",
    "            (ax1, train_mean_matrix, train_std_matrix, 'Training'),\n",
    "            (ax2, test_mean_matrix, test_std_matrix, 'Test')\n",
    "        ]:\n",
    "            sns.heatmap(mean_matrix, ax=ax,\n",
    "                    xticklabels=concepts,\n",
    "                    yticklabels=concepts,\n",
    "                    cmap='viridis',\n",
    "                    vmin=0.0,\n",
    "                    vmax=1.0,\n",
    "                    center=0.4,\n",
    "                    annot=False)  # We'll add custom annotations\n",
    "            \n",
    "            annotate_heatmap(ax, mean_matrix, std_matrix)\n",
    "            \n",
    "            ax.set_title(f'{title} {metric.replace(\"test_\", \"\").replace(\"_\", \" \").title()}')\n",
    "            ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "        plt.suptitle(\n",
    "            f'Train vs Test Performance for Concept Decoding\\nPatients {[p.pid for p in self.patient_data_list]}, Epoch: {self.epoch}\\n(mean ± std across {len(next(iter(self.results.values())))} iterations)',\n",
    "            y=1.05\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        #plt.show()\n",
    "\n",
    "        # Return statistics for analysis if needed\n",
    "        stats = {\n",
    "            'train_mean': train_mean_matrix,\n",
    "            'train_std': train_std_matrix,\n",
    "            'test_mean': test_mean_matrix,\n",
    "            'test_std': test_std_matrix\n",
    "        }\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p566 = PatientData(pid='566')\n",
    "p563 = PatientData(pid='563')\n",
    "p562 = PatientData(pid='562')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.1\n",
    "\n",
    "\n",
    "p566_fr_neurons = p566.filter_neurons_by_fr(neurons=p566.neurons, window=(p566.times_dict['movie_start_rel'], p566.times_dict['preSleep_recall_start_rel']), threshold=THRESHOLD)\n",
    "p566_mtl_fr_neurons = p566.filter_mtl_neurons(neurons=p566_fr_neurons)\n",
    "\n",
    "p563_fr_neurons = p563.filter_neurons_by_fr(neurons=p563.neurons, window=(p563.times_dict['movie_start_rel'], p563.times_dict['preSleep_recall_start_rel']), threshold=THRESHOLD)\n",
    "p563_mtl_fr_neurons = p563.filter_mtl_neurons(neurons=p563_fr_neurons)\n",
    "\n",
    "p562_fr_neurons = p562.filter_neurons_by_fr(neurons=p562.neurons, window=(p562.times_dict['movie_start_rel'], p562.times_dict['preSleep_recall_start_rel']), threshold=THRESHOLD)\n",
    "p562_mtl_fr_neurons = p562.filter_mtl_neurons(neurons=p562_fr_neurons)\n",
    "\n",
    "neurons_list = [p562_mtl_fr_neurons, p563_mtl_fr_neurons, p566_mtl_fr_neurons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p562_mtl_fr_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p566"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_pairs_to_decode = []\n",
    "concept_list = list(p566.movie_df.columns)\n",
    "#remove non-concept columns if they exist\n",
    "concept_list = [col for col in concept_list if col not in ['Frame', 'time_sec', 'rel_corrected_time_sec', 'uncorrected_time_sec']]\n",
    "\n",
    "\n",
    "# doing to 5 for time\n",
    "\n",
    "for i, concept1 in enumerate(concept_list[:]):\n",
    "    for concept2 in concept_list[i+1:]: #avoid duplicates and self-pairs\n",
    "        concept_pairs_to_decode.append((concept1, concept2))\n",
    "\n",
    "print(f\"Number of concept pairs to decode: {len(concept_pairs_to_decode)}\")\n",
    "print(concept_pairs_to_decode[:3]) # Print first 5 pairs as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_manager = MultiResultsManager(\n",
    "    patient_data_list=[p562, p563, p566],\n",
    "    concept_pairs=concept_pairs_to_decode,\n",
    "    epoch='movie',\n",
    "    standardize=False,\n",
    "    pseudo=True,\n",
    "    neurons_list=[p562_mtl_fr_neurons, p563_mtl_fr_neurons, p566_mtl_fr_neurons]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_manager.run_decoding_for_pairs(num_iter=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi_patient_heatmap(results_manager, metric='test_roc_auc', figsize=(20, 10), \n",
    "                              vmin=0.0, vmax=1.0, center=0.4, cmap='viridis',\n",
    "                              return_stats=True, save_path=None, \n",
    "                              selected_concepts=None, show_numbers=True):\n",
    "    \"\"\"\n",
    "    Generates a heatmap of decoding performance for a MultiResultsManager.\n",
    "    \n",
    "    Args:\n",
    "        results_manager: An instance of MultiResultsManager with results\n",
    "        metric (str): One of 'test_accuracy', 'train_accuracy', 'test_roc_auc', 'train_roc_auc'\n",
    "        figsize (tuple): Figure size for the plot\n",
    "        vmin, vmax, center: Color scale parameters for the heatmap\n",
    "        cmap: Colormap for the heatmap\n",
    "        return_stats (bool): Whether to return statistics dictionary\n",
    "        save_path (str): Optional path to save the figure\n",
    "        selected_concepts (list): List of specific concepts to include in heatmap (None = all)\n",
    "        show_numbers (bool): Whether to display performance numbers on the heatmap\n",
    "        \n",
    "    Returns:\n",
    "        stats (dict): Dictionary with performance matrices if return_stats is True\n",
    "    \"\"\"\n",
    "    # Get all unique concepts from the concept pairs\n",
    "    all_concepts = sorted(list(set([c for pair in results_manager.concept_pairs for c in pair])))\n",
    "    \n",
    "    # Filter concepts if selected_concepts is provided\n",
    "    if selected_concepts is not None:\n",
    "        # Ensure all selected concepts are valid\n",
    "        invalid_concepts = [c for c in selected_concepts if c not in all_concepts]\n",
    "        if invalid_concepts:\n",
    "            print(f\"Warning: These concepts are not in the results: {invalid_concepts}\")\n",
    "        \n",
    "        # Only keep concepts that are in both all_concepts and selected_concepts\n",
    "        concepts = sorted([c for c in all_concepts if c in selected_concepts])\n",
    "        \n",
    "        # Ensure we have at least 2 concepts\n",
    "        if len(concepts) < 2:\n",
    "            raise ValueError(\"Need at least 2 valid concepts to create a heatmap\")\n",
    "    else:\n",
    "        concepts = all_concepts\n",
    "    \n",
    "    n_concepts = len(concepts)\n",
    "    \n",
    "    # Initialize matrices for means and standard deviations\n",
    "    train_mean_matrix = np.full((n_concepts, n_concepts), np.nan)\n",
    "    test_mean_matrix = np.full((n_concepts, n_concepts), np.nan)\n",
    "    train_std_matrix = np.full((n_concepts, n_concepts), np.nan)\n",
    "    test_std_matrix = np.full((n_concepts, n_concepts), np.nan)\n",
    "\n",
    "    concept_to_idx = {concept: i for i, concept in enumerate(concepts)}\n",
    "\n",
    "    # Fill matrices with results\n",
    "    for concept_pair, results in results_manager.results.items():\n",
    "        c1, c2 = concept_pair\n",
    "        # Skip if either concept is not in our selected list\n",
    "        if c1 not in concepts or c2 not in concepts:\n",
    "            continue\n",
    "            \n",
    "        if results:  # Check if results exist for this pair\n",
    "            i, j = concept_to_idx[c1], concept_to_idx[c2]\n",
    "            \n",
    "            # Extract values for all iterations\n",
    "            if 'roc_auc' in metric:\n",
    "                train_values = [r.train_roc_auc for r in results]\n",
    "                test_values = [r.test_roc_auc for r in results]\n",
    "            else:  # accuracy\n",
    "                train_values = [r.train_accuracy for r in results]\n",
    "                test_values = [r.test_accuracy for r in results]\n",
    "            \n",
    "            # Calculate mean and std\n",
    "            train_mean = np.mean(train_values)\n",
    "            test_mean = np.mean(test_values)\n",
    "            train_std = np.std(train_values)\n",
    "            test_std = np.std(test_values)\n",
    "            \n",
    "            # Fill matrices symmetrically\n",
    "            for matrix, value in [(train_mean_matrix, train_mean), \n",
    "                                (test_mean_matrix, test_mean),\n",
    "                                (train_std_matrix, train_std),\n",
    "                                (test_std_matrix, test_std)]:\n",
    "                matrix[i, j] = value\n",
    "                matrix[j, i] = value\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    def annotate_heatmap(ax, mean_matrix, std_matrix):\n",
    "        \"\"\"Helper function to annotate heatmap with mean ± std\"\"\"\n",
    "        if not show_numbers:\n",
    "            return  # Skip annotation if show_numbers is False\n",
    "            \n",
    "        for i in range(mean_matrix.shape[0]):\n",
    "            for j in range(mean_matrix.shape[1]):\n",
    "                if not np.isnan(mean_matrix[i, j]):\n",
    "                    text = f'{mean_matrix[i, j]:.3f}\\n(±{std_matrix[i, j]:.3f})'\n",
    "                    ax.text(j + 0.5, i + 0.5, text,\n",
    "                        ha='center', va='center',\n",
    "                        color='white' if mean_matrix[i, j] > 0.5 else 'black',\n",
    "                        fontsize=8)\n",
    "                else:\n",
    "                    ax.text(j + 0.5, i + 0.5, 'N/A',\n",
    "                        ha='center', va='center',\n",
    "                        color='gray')\n",
    "\n",
    "    # Plot heatmaps\n",
    "    for ax, mean_matrix, std_matrix, title in [\n",
    "        (ax1, train_mean_matrix, train_std_matrix, 'Training'),\n",
    "        (ax2, test_mean_matrix, test_std_matrix, 'Test')\n",
    "    ]:\n",
    "        sns.heatmap(mean_matrix, ax=ax,\n",
    "                xticklabels=concepts,\n",
    "                yticklabels=concepts,\n",
    "                cmap=cmap,\n",
    "                vmin=vmin,\n",
    "                vmax=vmax,\n",
    "                center=center,\n",
    "                annot=False)  # We'll add custom annotations\n",
    "        \n",
    "        annotate_heatmap(ax, mean_matrix, std_matrix)\n",
    "        \n",
    "        ax.set_title(f'{title} {metric.replace(\"test_\", \"\").replace(\"_\", \" \").title()}')\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "    # Create title with patient information\n",
    "    if hasattr(results_manager, 'patient_data_list'):\n",
    "        # Multi-patient case\n",
    "        patient_ids = [p.pid for p in results_manager.patient_data_list]\n",
    "        patients_str = \", \".join(patient_ids)\n",
    "        title = f'Multi-Patient Decoding Performance\\nPatients: {patients_str}, Epoch: {results_manager.epoch}'\n",
    "    else:\n",
    "        # Single patient case (for compatibility)\n",
    "        title = f'Train vs Test Performance for Concept Decoding\\nPatient {results_manager.patient_data.pid}, Epoch: {results_manager.epoch}'\n",
    "    \n",
    "    # Add iteration info if available\n",
    "    if results_manager.results and next(iter(results_manager.results.values())):\n",
    "        first_result = next(iter(results_manager.results.values()))\n",
    "        title += f'\\n(mean ± std across {len(first_result)} iterations)'\n",
    "    \n",
    "    # Add concept selection info if applicable\n",
    "    if selected_concepts is not None:\n",
    "        title += f'\\n(Showing {len(concepts)} selected concepts)'\n",
    "        \n",
    "    plt.suptitle(title, y=1.05)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure if path provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    \n",
    "    # Return statistics if requested\n",
    "    if return_stats:\n",
    "        stats = {\n",
    "            'train_mean': train_mean_matrix,\n",
    "            'train_std': train_std_matrix,\n",
    "            'test_mean': test_mean_matrix,\n",
    "            'test_std': test_std_matrix,\n",
    "            'concepts': concepts,\n",
    "            'concept_to_idx': concept_to_idx\n",
    "        }\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multi_patient_heatmap(multi_manager, metric='test_accuracy', selected_concepts=best_concepts, show_numbers=False)\n",
    "plt.suptitle('Multi-Patient Psuedopopulation Character Decoding Performance\\nPatients: 562, 563, 566\\nMTL neurons only, above 0.1Hz Firing Rate', fontsize=17)\n",
    "plt.savefig('mtl_multipatient_without_acc')\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_concepts = ['A.Amar',\n",
    "  'A.Fayed',\n",
    "  'B.Buchanan',\n",
    "  'C.Manning',\n",
    "  'C.OBrian',\n",
    "  'J.Bauer',\n",
    "  'K.Hayes',\n",
    "  'M.OBrian',\n",
    "  'N.Yassir',\n",
    "  'R.Wallace',\n",
    "  'T.Lennox',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_concepts = [\n",
    "    \"A.Fayed\", \n",
    "    \"R.Wallace\", \n",
    "    \"T.Lennox\", \n",
    "    \"N.Yassir\", \n",
    "    \"K.Hayes\", \n",
    "    \"M.OBrian\", \n",
    "    \"J.Bauer\", \n",
    "    \"C.Manning\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now going to do decoding on combined neural data betwen patients, 0.1 neurons, all neurons not just mtl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
